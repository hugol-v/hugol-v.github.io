---
---

@misc{latourellevigeant2021gd,
  title={GD and Large Linear Regression: Concentration and Asymptotics for a Spiked Model}, 
  author={Hugo Latourelle-Vigeant},
  year={2021},
  month={08},
  note={4th Undergraduate Student Research Conference, McGill University},
  abstract={Machine learning algorithms are often trained using a large set of samples in a high-dimensional feature space. The traditional methods that performed well for low-dimensional problems can run into several problems in analyzing such high-dimensional models. One difficulty is that the analysis can depend on the probability distribution of the inputs. However, we show that this is not the case for a model of large-scale spiked random least squares problem trained with gradient descent. In fact, we show and verify experimentally that the halting time exhibits a universality property: it is independent of the probability distribution of the inputs. Furthermore, we provide explicit asymptotic results and discuss the effect of various parameters on those asymptotics.},
  slides={Spiked_models_slides.pdf},
  website={https://www.mcgill.ca/channels/channels/event/mathematics-statistics-4th-undergraduate-student-research-conference-332472},
  preview={halting_time.gif}
}

@misc{latourellevigeant2023cms,
  title={Matrix Dyson Equation for Correlated Linearizations},
  author={Hugo Latourelle-Vigeant},
  year={2023},
  month={12},
  note={Canadian Mathematical Society Winter Meeting, Montreal},
  abstract={The exploration of large random matrices through asymptotic deterministic equivalents has been approached by a multitude of techniques. One approach employs the matrix Dyson equation to establish an asymptotic equivalence between a random resolvent and the solution of a matrix fixed point equation. Another, the linearization trick, has proven effective in studying rational functions of random matrices. This trick involves embedding a matrix expression into a larger random matrix, known as a linear matrix pencil, with a simplified correlation structure. In this presentation, we introduce an extension of the matrix Dyson equation framework tailored specifically for linearizations. This extends previous work which has focused primarily on the case of pencils with blocks of canonical Wigner or Circular type. Within this framework, we derive an anisotropic global law for a broad class of pseudo-resolvents with general correlation structures. To highlight the practical implications of our framework, we apply it to a problem coming from machine learning. Specifically, we apply it to derive an exact asymptotic expression for the validation error of random features ridge regression and establish a general Gaussian equivalence result.},
  slides={cms-2023-slides.pdf},
  website={https://www2.cms.math.ca/Events/winter23/schedule_daily},
  preview={mastersthesis.gif}
}

@misc{latourellevigeant2023mde_linearization,
  title={Matrix Dyson Equation for Linearizations},
  author={Hugo Latourelle-Vigeant},
  year={2023},
  month={09},
  note={Seminar in random matrix theory, machine learning and optimization, McGill University},
  abstract={Random matrix theory has proved to be a valuable tool for understanding puzzling aspects of machine learning models. There are various methods in random matrix theory, each depending on different structural assumptions for the given problem. One of these methods essentially suggests that we can analyze the spectrum of a large random matrix by substituting the associated random resolvent with the solution of a deterministic fixed-point equation known as the matrix Dyson equation. In this talk, based on collaborative work with KC Tsiolis and Elliot Paquette, I will discuss the extension of the matrix Dyson equation framework to analyze rational expressions in random matrices using a linearization trick. I will also provide an illustrative example by applying our framework to study the test error of a random feature model.},
  website={https://elliotpaquette.github.io/rmtmloptseminar.html},
  preview={random_features.gif}
}