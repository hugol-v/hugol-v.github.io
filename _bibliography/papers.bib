---
---


@article{latourelle2026statisticalcomputationaltradeoffslearningmultiindex,
  bibtex_show={true},
  title={Statistical-Computational Trade-offs in Learning Multi-Index Models via Harmonic Analysis},
  author={Hugo Latourelle-Vigeant and Theodor Misiakiewicz},
  year={2026},
  eprint={2602.09959},
  archivePrefix={arXiv},
  primaryClass={math.ST},
  url={https://arxiv.org/abs/2602.09959}, 
  abbr={Preprint},
  preview={multi_index_harmonic.gif},
  selected={true},
  arxiv={2602.09959},
  abstract={We study the problem of learning multi-index models (MIMs), where the label depends on the input $x \in \mathbb{R}^d$ only through an unknown $s$-dimensional projection $W_*^{\top} x \in \mathbb{R}^s$. Exploiting the equivariance of this problem under the orthogonal group $O_d$, we obtain a sharp harmonic-analytic characterization of the learning complexity for MIMs with spherically symmetric inputs---which refines and generalizes previous Gaussian-specific analyses. Specifically, we derive statistical and computational complexity lower bounds within the Statistical Query (SQ) and Low-Degree Polynomial (LDP) frameworks. These bounds decompose naturally across spherical harmonic subspaces. Guided by this decomposition, we construct a family of spectral algorithms based on harmonic tensor unfolding that sequentially recover the latent directions and (nearly) achieve these SQ and LDP lower bounds. Depending on the choice of harmonic degree sequence, these estimators can realize a broad range of trade-offs between sample and runtime complexity. From a technical standpoint, our results build on the semisimple decomposition of the $O_d$-action on $L^2(S^{d-1})$ and the intertwining isomorphism between spherical harmonics and traceless symmetric tensors.}
}

@article{latourelle2026dyson,
  bibtex_show={true},
  title={Dyson Equation for Correlated Linearizations and Test Error of Random Features Regression},
  author={Hugo Latourelle-Vigeant and Elliot Paquette},
  year={2026},
  journal={Random Matrices: Theory and Applications},
  volume={15},
  number={01},
  pages={2550026},
  publisher={World Scientific},
  abbr={Journal},
  preview={random_features.gif},
  selected={true},
  article={https://www.worldscientific.com/doi/epdf/10.1142/S2010326325500261},
  arxiv={2312.09194v4},
  abstract={This paper develops some theory of the Dyson equation for correlated linearizations and uses it to solve a problem on asymptotic deterministic equivalent for the test error in random features regression. The theory developed for the correlated Dyson equation includes existence-uniqueness, spectral support bounds and stability properties. This theory is new for constructing deterministic equivalents for pseudo-resolvents of a class of linearizations with correlated entries. In the application, this theory is used to give a deterministic equivalent of the test error in random features ridge regression, in a proportional scaling regime, wherein we have conditioned on both training and test datasets.}
}

@mastersthesis{latourelle2024matrix,
  title={The matrix Dyson equation for machine learning: Correlated linearizations and the test error in random features regression},
  author={Hugo Latourelle-Vigeant},
  year={2024},
  publisher={McGill University},
  bibtex_show={true},
  selected={true},
  pdf={SZPMYTH_LATOURELLE-VIGEANT_HUGO_2024_THESIS_J249978549-j150441463.pdf},
  abbr={Thesis},
  eprint={https://escholarship.mcgill.ca/concern/theses/m900p100w?locale=en},
  article={https://escholarship.mcgill.ca/concern/theses/m900p100w?locale=en},
  preview={mastersthesis.gif},
  abstract={Contemporary machine learning models, particularly deep learning models, are frequently trained on large datasets within high-dimensional feature spaces, presenting challenges for traditional analytical approaches. Notably, the effective generalization of highly overparameterized models contradicts conventional statistical wisdom. Furthermore, the presence of non-linear activations in artificial neural networks adds complexity to their analysis. To simplify theoretical analysis, it is often assumed that training data is sampled from an unstructured distribution. While such analyses offer insights into certain aspects of machine learning, they fall short in elucidating how neural networks extract information from the structure of the data, crucial for their success in real-world applications. Fortunately, random matrix theory has emerged as a valuable tool for theoretically understanding certain machine learning procedures. Various techniques have been employed to explore large random matrices through asymptotic deterministic equivalents. One such approach involves substituting the random resolvent associated with a large random matrix with the solution of a deterministic fixed-point equation known as the matrix Dyson equation. Another effective technique, known as the linearization trick, involves embedding a matrix expression into a larger random matrix, termed a linear matrix pencil, with a simplified correlation structure. In this thesis, we extend the matrix Dyson equation framework to derive an anisotropic global law for a broad class of pseudo-resolvents with general correlation structures. This extension enables the analysis of spectral properties of a wide range of random matrices using a simpler and deterministic solution to the matrix Dyson equation. Through the development of this theory, we address critical aspects such as existence-uniqueness, spectral support bounds, and stability properties. These considerations are essential for constructing deterministic equivalents for pseudo-resolvents of a class of correlated linear pencils. Leveraging this theoretical framework, we provide an asymptotically exact deterministic expression for the empirical test error of random features ridge regression. The random features model, characterized by its non-linear activation function and potential for overparameterization, emerges as a powerful model for studying phenomena observed in real-life machine learning models, such as multiple descent and implicit regularization. Our exact expression facilitates a precise characterization of the implicit regularization of the model and unveils connections between random features regression and closely related kernel methods. Since we make no particular assumptions about the distribution of the data and response variable, our work represents a significant step towards understanding how neural networks exploit specific data structures.}
}

