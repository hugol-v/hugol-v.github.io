{
  "basics": {
    "name": "Hugo Latourelle-Vigeant",
    "label": "",
    "image": "",
    "email": "",
    "phone": "",
    "url": "",
    "summary": "",
    "location": {
      "address": "",
      "postalCode": "",
      "city": "",
      "countryCode": "",
      "region": ""
    },
    "profiles": [
      {
        "network": "",
        "username": "",
        "url": ""
      }
    ]
  },
  "education": [
    {
      "institution": "Yale University",
      "location": "Connecticut, USA",
      "url": "https://statistics.yale.edu/",
      "area": "",
      "studyType": "Ph.D. Statistics and Data Science",
      "startDate": "2024",
      "endDate": "",
      "score": "",
      "courses": []
    },
    {
      "institution": "McGill University",
      "location": "Montreal, Canada",
      "url": "https://www.mcgill.ca/mathstat/",
      "area": "",
      "studyType": "M.Sc. Mathematics and Statistics",
      "startDate": "2022",
      "endDate": "2024",
      "score": "",
      "courses": ["Master's Thesis: The matrix Dyson equation for machine learning: Correlated linearizations and the test error in random features regression"]
    },
    {
      "institution": "McGill University",
      "location": "Montreal, Canada",
      "url": "https://www.mcgill.ca/",
      "area": "",
      "studyType": "B.Sc. Joint Honours Mathematics and Computer Science",
      "startDate": "2018",
      "endDate": "2022",
      "score": "",
      "courses": ["Graduated with First Class Joint Honours"]
    }
  ],
  "work": [
    {
      "name": "CDPQ",
      "location": "Montreal, Canada",
      "position": "Data Science Intern",
      "url": "https://www.cdpq.com/en",
      "startDate": "2024",
      "endDate": "",
      "summary": "Internship  in data science at CDPQ during the summer of 2024. Part of the NLP team.",
      "highlights": []
    }
  ],
  "publications": [
    {
      "name": "The matrix Dyson equation for machine learning: Correlated linearizations and the test error in random features regression",
      "publisher": "McGill University",
      "releaseDate": "2024",
      "authors": "Hugo Latourelle-Vigeant",
      "url": "https://escholarship.mcgill.ca/concern/theses/m900p100w?locale=en",
      "type": "Research",
      "summary": "Contemporary machine learning models, particularly deep learning models, are frequently trained on large datasets within high-dimensional feature spaces, presenting challenges for traditional analytical approaches. Notably, the effective generalization of highly overparametrized models contradicts conventional statistical wisdom. Furthermore, the presence of non-linear activations in artificial neural networks adds complexity to their analysis. To simplify theoretical analysis, it is often assumed that training data is sampled from an unstructured distribution. While such analyses offer insights into certain aspects of machine learning, they fall short in elucidating how neural networks extract information from the structure of the data, crucial for their success in real-world applications.Fortunately, random matrix theory has emerged as a valuable tool for theoretically understanding certain machine learning procedures. Various techniques have been employed to explore large random matrices through asymptotic deterministic equivalents. One such approach involves substituting the random resolvent associated with a large random matrix with the solution of a deterministic fixed-point equation known as the matrix Dyson equation. Another effective technique, known as the linearization trick, involves embedding a matrix expression into a larger random matrix, termed a linear matrix pencil, with a simplified correlation structure.In this thesis, we extend the matrix Dyson equation framework to derive an anisotropic global law for a broad class of pseudo-resolvents with general correlation structures. This extension enables the analysis of spectral properties of a wide range of random matrices using a simpler and deterministic solution to the matrix Dyson equation. Through the development of this theory, we address critical aspects such as existence-uniqueness, spectral support bounds, and stability properties. These considerations are essential for constructing deterministic equivalents for pseudo-resolvents of a class of correlated linear pencils.Leveraging this theoretical framework, we provide an asymptotically exact deterministic expression for the empirical test error of random features ridge regression. The random features model, characterized by its non-linear activation function and potential for overparametrization, emerges as a powerful model for studying phenomena observed in real-life machine learning models, such as multiple descent and implicit regularization. Our exact expression facilitates a precise characterization of the implicit regularization of the model and unveils connections between random features regression and closely related kernel methods. Since we make no particular assumptions about the distribution of the data and response variable, our work represents a significant step towards understanding how neural networks exploit specific data structures."
    },
    {
      "name": "Matrix Dyson equation for correlated linearizations and test error of random features regression",
      "publisher": "Preprint: arXiv:2312.09194",
      "releaseDate": "2023",
      "authors": "Hugo Latourelle-Vigeant and Elliot Paquette",
      "url": "https://arxiv.org/pdf/2312.09194",
      "type": "Research",
      "summary": "This paper develops some theory of the matrix Dyson equation (MDE) for correlated linearizations and uses it to solve a problem on asymptotic deterministic equivalent for the test error in random features regression. The theory developed for the correlated MDE includes existence-uniqueness, spectral support bounds, and stability properties of the MDE. This theory is new for constructing deterministic equivalents for pseudoresolvents of a class of correlated linear pencils. In the application, this theory is used to give a deterministic equivalent of the test error in random features ridge regression, in a proportional scaling regime, wherein we have conditioned on both training and test datasets."
    }
  ],
  "presentations": [
    {
      "name": "Matrix Dyson Equation for Correlated Linearizations",
      "date": "2023-12-03",
      "url": "",
      "venue": "The many facets of random matrix theory Workshop at Canadian Mathematical Society Winter Meeting",
      "location": "Montreal, Canada",
      "summary": "The exploration of large random matrices through asymptotic deterministic equivalents has been approached by a multitude of techniques. One approach employs the matrix Dyson equation to establish an asymptotic equivalence between a random resolvent and the solution of a matrix fixed point equation. Another, the linearization trick, has proven effective in studying rational functions of random matrices. This trick involves embedding a matrix expression into a larger random matrix, known as a linear matrix pencil, with a simplified correlation structure.\n\nIn this presentation, we introduce an extension of the matrix Dyson equation framework tailored specifically for linearizations. This extends previous work which has focused primarily on the case of pencils with blocks of canonical Wigner or Circular type. Within this framework, we derive an anisotropic global law for a broad class of pseudo-resolvents with general correlation structures. To highlight the practical implications of our framework, we apply it to a problem coming from machine learning. Specifically, we apply it to derive an exact asymptotic expression for the validation error of random features ridge regression and establish a general Gaussian equivalence result."
    },
    {
      "name": "Matrix Dyson Equation for Linearizations",
      "date": "2023-09-06",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "Random matrix theory has proved to be a valuable tool for understanding puzzling aspects of machine learning models. There are various methods in random matrix theory, each depending on different structural assumptions for the given problem. One of these methods essentially suggests that we can analyze the spectrum of a large random matrix by substituting the associated random resolvent with the solution of a deterministic fixed-point equation known as the matrix Dyson equation. In this talk, based on collaborative work with KC Tsiolis and Elliot Paquette, I will discuss the extension of the matrix Dyson equation framework to analyze rational expressions in random matrices using a linearization trick. I will also provide an illustrative example by applying our framework to study the test error of a random feature model."
    },
    {
      "name": "GD and Large Linear Regression: Concentration and Asymptotics for a Spiked Model",
      "date": "2021-08-23",
      "url": "",
      "venue": "4th Undergraduate Student Research Conference at McGill University",
      "location": "Montreal, Canada",
      "summary": "Machine learning algorithms are often trained using a large set of samples in a high-dimensional feature space. The traditional methods that performed well for low-dimensional problems can run into several problems in analyzing such high-dimensional models. One difficulty is that the analysis can depend on the probability distribution of the inputs. However, we show that this is not the case for a model of large-scale spiked random least squares problem trained with gradient descent. In fact, we show and verify experimentally that the halting time exhibits a universality property: it is independent of the probability distribution of the inputs. Furthermore, we provide explicit asymptotic results and discuss the effect of various parameters on those asymptotics."
    }
  ],
  "teaching": [
    {
      "course": "Calculus 2 - MATH 141",
      "location": "McGill University",
      "position": "Teaching Assistant",
      "semester": "Winter 2024",
      "date": "2024-01",
      "highlights": []
    },
    {
      "course": "Convex Optimization - Math 463/563",
      "location": "McGill University",
      "position": "Graduate Course Assistant",
      "semester": "Winter 2024",
      "date": "2024-01",
      "highlights": []
    },
    {
      "course": "Calculus 2 - MATH 141",
      "location": "McGill University",
      "position": "Teaching Assistant",
      "semester": "Winter 2023",
      "date": "2023-01",
      "highlights": []
    },
    {
      "course": "Convex Optimization - Math 463/563",
      "location": "McGill University",
      "position": "Graduate Course Assistant",
      "semester": "Winter 2023",
      "date": "2023-01",
      "highlights": []
    },
    {
      "course": "Calculus 2 - MATH 141",
      "location": "McGill University",
      "position": "Teaching Assistant",
      "semester": "Fall 2022",
      "date": "2022-09",
      "highlights": []
    },
    {
      "course": "Numerical Optimization - Math 560",
      "location": "McGill University",
      "position": "Graduate Course Assistant",
      "semester": "Fall 2022",
      "date": "2022-09",
      "highlights": []
    },
    {
      "course": "Numerical Optimization - Math 560",
      "location": "McGill University",
      "position": "Undergraduate Course Assistant",
      "semester": "Winter 2022",
      "date": "2022-01",
      "highlights": []
    }
  ],
  "awards": [
    {
      "title": "First-class honours in Mathematics and Computer Science",
      "date": "2022",
      "awarder": "McGill University",
      "url": "",
      "summary": ""
    },
    {
      "title": "Undergraduate student research award",
      "date": "2021",
      "awarder": "NSERC",
      "url": "https://www.nserc-crsng.gc.ca/students-etudiants/ug-pc/usra-brpc_eng.asp",
      "summary": "The NSERC Undergraduate Student Research Award is a competitive award granted by the Natural Sciences and Engineering Research Council of Canada (NSERC) on the basis of academic excellence and research potential to support a full-time undergraduate summer research project."
    },
    {
      "title": "Major entrance scholarship in science",
      "date": "2018",
      "awarder": "Hydro-Québec",
      "url": "",
      "summary": ""
    }
  ],
  "organizer": [
    {
      "name": "Montreal RMT-ML-OPT seminar at McGill University",
      "url": "https://elliotpaquette.github.io/rmtmloptseminar.html",
      "date": "Fall 2023"
    }
  ],
  "reviewer": [
    {
      "conference": "ICML",
      "date": "2024",
      "workshop": "2nd Workshop on High-dimensional Learning Dynamics (HiLD): The Emergence of Structure and Reasoning",
      "url": "https://sites.google.com/view/hidimlearning/home?pli=1"
    },
    {
      "conference": "NeurIPS",
      "date": "2023",
      "workshop": "OPT Workshop on Optimization for Machine Learning",
      "url": "https://opt-ml.org/"
    },
    {
      "conference": "ICML",
      "date": "2023",
      "workshop": "High-dimensional Learning Dynamics Workshop",
      "url": "https://sites.google.com/view/hidimlearning23/home"
    },
    {
      "conference": "NeurIPS",
      "date": "2022",
      "workshop": "OPT Workshop on Optimization for Machine Learning",
      "url": "https://opt-ml.org/oldopt/opt22/"
    }
  ],
  "litterature_review": [
    {
      "name": "Illustrating High-Dimensional Limit Theorems for SGD via Examples",
      "date": "2023-04-29",
      "type": "review",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "This presentation provides an overview of the article <a href='https://arxiv.org/pdf/2206.04030.pdf'>“High-Dimensional Limit Theorems for SGD: Effective Dynamics and Critical Scaling”</a> by Gérard Ben Arous, Reza Gheissari, and Aukosh Jagannath. The article investigates the behavior of Stochastic Gradient Descent (SGD) with constant step-size in high-dimensional settings. The authors demonstrate that the online stochastic gradient trajectory can be approximated by a system of stochastic differential equations (SDEs). To help illustrate the main concepts of the paper, I will go through examples of applying SGD to a spiked matrix PCA problem and a two-layer network for binary Gaussian mixture classification."
    },
    {
      "name": "Using Random Matrix Models to Predict How Real-World Neural Representations Generalize",
      "date": "2022-09-14",
      "type": "review",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "How can we predict the generalization risk of overparametrized large-scale machine learning models? Even for simple but realistic regression problems, traditional theoretical analyses geared toward answering this question fail to capture some important qualitative behavior. Fortunately, the generalized cross validation (GCV) estimator can be used to accurately estimate the generalization risk. Based on the paper <a href='https://arxiv.org/abs/2203.06176'>“More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize”</a> by Wei, Hu and Steinhardt, I will explain how the GCV predicts generalization risk while other common methods fail to do the same. If time permits, I will also state some implications of this result, notably on pretraining."
    },
    {
      "name": "Hessian Eigenspectra of G-GLMs",
      "date": "2021-10-01",
      "type": "review",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "Hessian matrices are crucial to the study of optimization and machine learning algorithms. Based on <a href='https://arxiv.org/abs/2103.01519'>the work of Zhenyu Liao and Michael W. Mahoney</a>, I will present an exact asymptotic characterization of the Hessian eigenspectrum for a large family of statistical models in high dimensions. More precisely, I will introduce a limiting spectral measure theorem and discuss its implications."
    }
  ]
}
