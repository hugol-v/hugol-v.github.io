{
  "basics": {
    "name": "Hugo Latourelle-Vigeant",
    "label": "",
    "image": "",
    "email": "",
    "phone": "",
    "url": "",
    "summary": "",
    "location": {
      "address": "",
      "postalCode": "",
      "city": "",
      "countryCode": "",
      "region": ""
    },
    "profiles": [
      {
        "network": "",
        "username": "",
        "url": ""
      }
    ]
  },
  "education": [
    {
      "institution": "Yale University",
      "location": "Connecticut, USA",
      "url": "https://statistics.yale.edu/",
      "area": "",
      "studyType": "Ph.D. Statistics and Data Science",
      "startDate": "2024",
      "endDate": "",
      "score": "",
      "courses": []
    },
    {
      "institution": "McGill University",
      "location": "Montreal, Canada",
      "url": "https://www.mcgill.ca/mathstat/",
      "area": "",
      "studyType": "M.Sc. Mathematics and Statistics",
      "startDate": "2022",
      "endDate": "2024",
      "score": "",
      "courses": ["Master's Thesis: The matrix Dyson equation for machine learning: Correlated linearizations and the test error in random features regression"]
    },
    {
      "institution": "McGill University",
      "location": "Montreal, Canada",
      "url": "https://www.mcgill.ca/",
      "area": "",
      "studyType": "B.Sc. Joint Honours Mathematics and Computer Science",
      "startDate": "2018",
      "endDate": "2022",
      "score": "",
      "courses": ["Graduated with First Class Joint Honours"]
    }
  ],
  "work": [
    {
      "name": "CDPQ",
      "location": "Montreal, Canada",
      "position": "Data Science Intern",
      "url": "https://www.cdpq.com/en",
      "startDate": "2024",
      "endDate": "",
      "summary": "Internship  in data science at CDPQ during the summer of 2024. Part of the NLP team.",
      "highlights": []
    }
  ],
  "publications": [
    {
      "name": "The matrix Dyson equation for machine learning: Correlated linearizations and the test error in random features regression",
      "publisher": "McGill University",
      "releaseDate": "2024",
      "authors": "Hugo Latourelle-Vigeant",
      "url": "https://escholarship.mcgill.ca/concern/theses/m900p100w?locale=en",
      "type": "Research",
      "summary": "Extended the matrix Dyson equation framework to derive an anisotropic global law for pseudo-resolvents with general correlation structures. Applied this framework to provide an exact deterministic expression for the empirical test error in random features ridge regression, addressing aspects such as existence-uniqueness, spectral support bounds, and stability properties."
    },
    {
      "name": "Matrix Dyson equation for correlated linearizations and test error of random features regression",
      "publisher": "Preprint: arXiv:2312.09194",
      "releaseDate": "2023",
      "authors": "Hugo Latourelle-Vigeant and Elliot Paquette",
      "url": "https://arxiv.org/pdf/2312.09194",
      "type": "Research",
      "summary": "Developed theory for the matrix Dyson equation for correlated linearizations, including existence-uniqueness, spectral support bounds, and stability properties, and applied this to derive a deterministic equivalent for the test error in random features ridge regression."
    }
  ],
  "presentations": [
    {
      "name": "Matrix Dyson Equation for Correlated Linearizations",
      "date": "2023-12-03",
      "url": "",
      "venue": "The many facets of random matrix theory Workshop at Canadian Mathematical Society Winter Meeting",
      "location": "Montreal, Canada",
      "summary": "Extended the matrix Dyson equation framework for linearizations to derive an anisotropic global law for pseudo-resolvents with general correlation structures, and applied this to derive an exact asymptotic expression for the validation error of random features ridge regression."
    },
    {
      "name": "Matrix Dyson Equation for Linearizations",
      "date": "2023-09-06",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "Extended the matrix Dyson equation framework to analyze rational expressions in random matrices using a linearization trick, and applied this to study the test error of a random feature model."
    },
    {
      "name": "GD and Large Linear Regression: Concentration and Asymptotics for a Spiked Model",
      "date": "2021-08-23",
      "url": "",
      "venue": "4th Undergraduate Student Research Conference at McGill University",
      "location": "Montreal, Canada",
      "summary": "Demonstrated that the halting time in large-scale spiked random least squares problems trained with gradient descent exhibits a universality property, independent of input probability distribution, and provided explicit asymptotic results."
    }
  ],
  "teaching": [
    {
      "course": "Calculus 2 - MATH 141",
      "location": "McGill University",
      "position": "Teaching Assistant",
      "semester": "Winter 2024",
      "date": "2024-01",
      "highlights": []
    },
    {
      "course": "Convex Optimization - Math 463/563",
      "location": "McGill University",
      "position": "Graduate Course Assistant",
      "semester": "Winter 2024",
      "date": "2024-01",
      "highlights": []
    },
    {
      "course": "Calculus 2 - MATH 141",
      "location": "McGill University",
      "position": "Teaching Assistant",
      "semester": "Winter 2023",
      "date": "2023-01",
      "highlights": []
    },
    {
      "course": "Convex Optimization - Math 463/563",
      "location": "McGill University",
      "position": "Graduate Course Assistant",
      "semester": "Winter 2023",
      "date": "2023-01",
      "highlights": []
    },
    {
      "course": "Calculus 2 - MATH 141",
      "location": "McGill University",
      "position": "Teaching Assistant",
      "semester": "Fall 2022",
      "date": "2022-09",
      "highlights": []
    },
    {
      "course": "Numerical Optimization - Math 560",
      "location": "McGill University",
      "position": "Graduate Course Assistant",
      "semester": "Fall 2022",
      "date": "2022-09",
      "highlights": []
    },
    {
      "course": "Numerical Optimization - Math 560",
      "location": "McGill University",
      "position": "Undergraduate Course Assistant",
      "semester": "Winter 2022",
      "date": "2022-01",
      "highlights": []
    }
  ],
  "awards": [
    {
      "title": "First-class honours in Mathematics and Computer Science",
      "date": "2022",
      "awarder": "McGill University",
      "url": "",
      "summary": ""
    },
    {
      "title": "Undergraduate student research award",
      "date": "2021",
      "awarder": "NSERC",
      "url": "https://www.nserc-crsng.gc.ca/students-etudiants/ug-pc/usra-brpc_eng.asp",
      "summary": "The NSERC Undergraduate Student Research Award is a competitive award granted by the Natural Sciences and Engineering Research Council of Canada (NSERC) on the basis of academic excellence and research potential to support a full-time undergraduate summer research project."
    },
    {
      "title": "Major entrance scholarship in science",
      "date": "2018",
      "awarder": "Hydro-Québec",
      "url": "",
      "summary": ""
    }
  ],
  "organizer": [
    {
      "name": "Montreal RMT-ML-OPT seminar at McGill University",
      "url": "https://elliotpaquette.github.io/rmtmloptseminar.html",
      "date": "Fall 2023"
    }
  ],
  "reviewer": [
    {
      "conference": "ICML",
      "date": "2024",
      "workshop": "2nd Workshop on High-dimensional Learning Dynamics (HiLD): The Emergence of Structure and Reasoning",
      "url": "https://sites.google.com/view/hidimlearning/home?pli=1"
    },
    {
      "conference": "NeurIPS",
      "date": "2023",
      "workshop": "OPT Workshop on Optimization for Machine Learning",
      "url": "https://opt-ml.org/"
    },
    {
      "conference": "ICML",
      "date": "2023",
      "workshop": "High-dimensional Learning Dynamics Workshop",
      "url": "https://sites.google.com/view/hidimlearning23/home"
    },
    {
      "conference": "NeurIPS",
      "date": "2022",
      "workshop": "OPT Workshop on Optimization for Machine Learning",
      "url": "https://opt-ml.org/oldopt/opt22/"
    }
  ],
  "litterature_review": [
    {
      "name": "Illustrating High-Dimensional Limit Theorems for SGD via Examples",
      "date": "2023-04-29",
      "type": "review",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "This presentation provides an overview of the article <a href='https://arxiv.org/pdf/2206.04030.pdf'>“High-Dimensional Limit Theorems for SGD: Effective Dynamics and Critical Scaling”</a> by Gérard Ben Arous, Reza Gheissari, and Aukosh Jagannath. The article investigates the behavior of Stochastic Gradient Descent (SGD) with constant step-size in high-dimensional settings. The authors demonstrate that the online stochastic gradient trajectory can be approximated by a system of stochastic differential equations (SDEs). To help illustrate the main concepts of the paper, I will go through examples of applying SGD to a spiked matrix PCA problem and a two-layer network for binary Gaussian mixture classification."
    },
    {
      "name": "Using Random Matrix Models to Predict How Real-World Neural Representations Generalize",
      "date": "2022-09-14",
      "type": "review",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "How can we predict the generalization risk of overparametrized large-scale machine learning models? Even for simple but realistic regression problems, traditional theoretical analyses geared toward answering this question fail to capture some important qualitative behavior. Fortunately, the generalized cross validation (GCV) estimator can be used to accurately estimate the generalization risk. Based on the paper <a href='https://arxiv.org/abs/2203.06176'>“More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize”</a> by Wei, Hu and Steinhardt, I will explain how the GCV predicts generalization risk while other common methods fail to do the same. If time permits, I will also state some implications of this result, notably on pretraining."
    },
    {
      "name": "Hessian Eigenspectra of G-GLMs",
      "date": "2021-10-01",
      "type": "review",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "Hessian matrices are crucial to the study of optimization and machine learning algorithms. Based on <a href='https://arxiv.org/abs/2103.01519'>the work of Zhenyu Liao and Michael W. Mahoney</a>, I will present an exact asymptotic characterization of the Hessian eigenspectrum for a large family of statistical models in high dimensions. More precisely, I will introduce a limiting spectral measure theorem and discuss its implications."
    }
  ]
}
