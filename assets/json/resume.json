{
  "basics": {
    "name": "Hugo Latourelle-Vigeant",
    "label": "",
    "image": "",
    "email": "",
    "phone": "",
    "url": "",
    "summary": "",
    "location": {
      "address": "",
      "postalCode": "",
      "city": "",
      "countryCode": "",
      "region": ""
    },
    "profiles": [
      {
        "network": "",
        "username": "",
        "url": ""
      }
    ]
  },
  "education": [
    {
      "institution": "Yale University",
      "location": "Connecticut, USA",
      "url": "https://statistics.yale.edu/",
      "area": "",
      "studyType": "Ph.D. Statistics and Data Science",
      "startDate": "2024",
      "endDate": "",
      "score": "",
      "courses": []
    },
    {
      "institution": "McGill University",
      "location": "Montreal, Canada",
      "url": "https://www.mcgill.ca/mathstat/",
      "area": "",
      "studyType": "M.Sc. Mathematics and Statistics",
      "startDate": "2022",
      "endDate": "2024",
      "score": "",
      "courses": ["Master's Thesis: The matrix Dyson equation for machine learning: Correlated linearizations and the test error in random features regression"]
    },
    {
      "institution": "McGill University",
      "location": "Montreal, Canada",
      "url": "https://www.mcgill.ca/",
      "area": "",
      "studyType": "B.Sc. Joint Honours Mathematics and Computer Science",
      "startDate": "2018",
      "endDate": "2022",
      "score": "",
      "courses": ["Graduated with First Class Joint Honours"]
    }
  ],
  "work": [
    {
      "name": "CDPQ",
      "location": "Montreal, Canada",
      "position": "Data Science Intern",
      "url": "https://www.cdpq.com/en",
      "startDate": "2024",
      "endDate": "",
      "summary": "Internship  in data science at CDPQ during the summer of 2024. Part of the NLP team.",
      "highlights": []
    }
  ],
  "publications": [
    {
      "name": "Statistical-Computational Trade-offs in Learning Multi-Index Models via Harmonic Analysis",
      "publisher": "Preprint: arXiv:2602.09959",
      "releaseDate": "2026",
      "authors": "Hugo Latourelle-Vigeant and Theodor Misiakiewicz",
      "url": "https://arxiv.org/abs/2602.09959",
      "type": "Research",
      "summary": "We study the problem of learning multi-index models (MIMs), where the label depends on the input only through an unknown low-dimensional projection. Exploiting the equivariance of this problem under the orthogonal group, we obtain a sharp harmonic-analytic characterization of the learning complexity for MIMs with spherically symmetric inputs, refining and generalizing previous Gaussian-specific analyses. We derive statistical and computational lower bounds in the Statistical Query and Low-Degree Polynomial frameworks that decompose across spherical harmonic subspaces. Guided by this structure, we construct spectral algorithms based on harmonic tensor unfolding that sequentially recover the latent directions and nearly achieve these bounds, enabling a range of trade-offs between sample and runtime complexity."
    },

    {
      "name": "Dyson Equation for Correlated Linearizations and Test Error of Random Features Regression",
      "publisher": "Random Matrices: Theory and Applications",
      "releaseDate": "2026",
      "authors": "Hugo Latourelle-Vigeant and Elliot Paquette",
      "url": "https://doi.org/10.1142/S2010326325500261",
      "type": "Research",
      "summary": "Developed a theory of the matrix Dyson equation for correlated linearizations, including existence-uniqueness, spectral support bounds, and stability properties. This framework is applied to derive a deterministic equivalent for the empirical test error in random features ridge regression in a proportional high-dimensional regime, conditioned on both training and test data. The results provide a rigorous understanding of generalization in random features models and establish a Gaussian equivalence principle for the test error."
    },

    {
      "name": "The matrix Dyson equation for machine learning: Correlated linearizations and the test error in random features regression",
      "publisher": "McGill University",
      "releaseDate": "2024",
      "authors": "Hugo Latourelle-Vigeant",
      "url": "https://escholarship.mcgill.ca/concern/theses/m900p100w?locale=en",
      "type": "Thesis",
      "summary": "Extended the matrix Dyson equation framework to an anisotropic global law for pseudo-resolvents with general correlation structures. The thesis develops existence-uniqueness, spectral support, and stability theory for correlated linear pencils, and applies this machinery to obtain an asymptotically exact deterministic expression for the test error of random features ridge regression. This work clarifies the role of implicit regularization in random features models and their connection with kernel methods, without assuming specific data distributions."
    }
  ]
  ,
  "presentations": [
    {
      "name": "Matrix Dyson Equation for Correlated Linearizations",
      "date": "2023-12-03",
      "url": "",
      "venue": "The many facets of random matrix theory Workshop at Canadian Mathematical Society Winter Meeting",
      "location": "Montreal, Canada",
      "summary": "Extended the matrix Dyson equation framework for linearizations to derive an anisotropic global law for pseudo-resolvents with general correlation structures, and applied this to derive an exact asymptotic expression for the validation error of random features ridge regression."
    },
    {
      "name": "Matrix Dyson Equation for Linearizations",
      "date": "2023-09-06",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "Extended the matrix Dyson equation framework to analyze rational expressions in random matrices using a linearization trick, and applied this to study the test error of a random feature model."
    },
    {
      "name": "GD and Large Linear Regression: Concentration and Asymptotics for a Spiked Model",
      "date": "2021-08-23",
      "url": "",
      "venue": "4th Undergraduate Student Research Conference at McGill University",
      "location": "Montreal, Canada",
      "summary": "Demonstrated that the halting time in large-scale spiked random least squares problems trained with gradient descent exhibits a universality property, independent of input probability distribution, and provided explicit asymptotic results."
    }
  ],
  "teaching": [
    {
      "course_name": "Theory of Statistics",
      "course_number": "S&DS2420",
      "department": "Department of Statistics and Data Science",
      "university": "Yale University",
      "position": "Teaching Fellow",
      "semester": "Spring 2026",
      "date": "2026-01",
      "highlights": []
    },
    {
      "course_name": "Probability",
      "course_number": "S&DS2410",
      "department": "Department of Statistics and Data Science",
      "university": "Yale University",
      "position": "Head Teaching Fellow",
      "semester": "Fall 2025",
      "date": "2025-09",
      "highlights": []
    },

    {
    "course_name": "Calculus 2",
    "course_number": "MATH 141",
    "department": "Department of Mathematics and Statistics",
    "university": "McGill University",
    "position": "Teaching Assistant",
    "semester": "Winter 2024",
    "date": "2024-01",
    "highlights": []
  },
  {
    "course_name": "Convex Optimization",
    "course_number": "MATH 463/563",
    "department": "Department of Mathematics and Statistics",
    "university": "McGill University",
    "position": "Graduate Course Assistant",
    "semester": "Winter 2024",
    "date": "2024-01",
    "highlights": []
  },
  {
    "course_name": "Calculus 2",
    "course_number": "MATH 141",
    "department": "Department of Mathematics and Statistics",
    "university": "McGill University",
    "position": "Teaching Assistant",
    "semester": "Winter 2023",
    "date": "2023-01",
    "highlights": []
  },
  {
    "course_name": "Convex Optimization",
    "course_number": "MATH 463/563",
    "department": "Department of Mathematics and Statistics",
    "university": "McGill University",
    "position": "Graduate Course Assistant",
    "semester": "Winter 2023",
    "date": "2023-01",
    "highlights": []
  },
  {
    "course_name": "Calculus 2",
    "course_number": "MATH 141",
    "department": "Department of Mathematics and Statistics",
    "university": "McGill University",
    "position": "Teaching Assistant",
    "semester": "Fall 2022",
    "date": "2022-09",
    "highlights": []
  },
  {
    "course_name": "Numerical Optimization",
    "course_number": "MATH 560",
    "department": "Department of Mathematics and Statistics",
    "university": "McGill University",
    "position": "Graduate Course Assistant",
    "semester": "Fall 2022",
    "date": "2022-09",
    "highlights": []
  },
  {
    "course_name": "Numerical Optimization",
    "course_number": "MATH 560",
    "department": "Department of Mathematics and Statistics",
    "university": "McGill University",
    "position": "Undergraduate Course Assistant",
    "semester": "Winter 2022",
    "date": "2022-01",
    "highlights": []
  }
  ],
  "awards": [
    {
      "title": "First-class honours in Mathematics and Computer Science",
      "date": "2022",
      "awarder": "McGill University",
      "url": "",
      "summary": ""
    },
    {
      "title": "Undergraduate student research award",
      "date": "2021",
      "awarder": "NSERC",
      "url": "https://www.nserc-crsng.gc.ca/students-etudiants/ug-pc/usra-brpc_eng.asp",
      "summary": "The NSERC Undergraduate Student Research Award is a competitive award granted by the Natural Sciences and Engineering Research Council of Canada (NSERC) on the basis of academic excellence and research potential to support a full-time undergraduate summer research project."
    },
    {
      "title": "Major entrance scholarship in science",
      "date": "2018",
      "awarder": "Hydro-Québec",
      "url": "",
      "summary": ""
    }
  ],
  "organizer": [
    {
      "name": "Montreal RMT-ML-OPT seminar at McGill University",
      "url": "https://elliotpaquette.github.io/rmtmloptseminar.html",
      "date": "Fall 2023"
    }
  ],
  "reviewer": [
    {
      "conference": "ICML",
      "date": "2025",
      "workshop": "3rd Workshop on High-dimensional Learning Dynamics (HiLD)",
      "url": "https://sites.google.com/view/hidimlearning/home"
    },
    {
      "conference": "NeurIPS",
      "date": "2024",
      "workshop": "16th International OPT Workshop on Optimization for Machine Learning",
      "url": "https://opt-ml.org/"
    },
    {
      "conference": "ICML",
      "date": "2024",
      "workshop": "2nd Workshop on High-dimensional Learning Dynamics (HiLD): The Emergence of Structure and Reasoning",
      "url": "https://sites.google.com/view/hidimlearning/home"
    },
    {
      "conference": "NeurIPS",
      "date": "2023",
      "workshop": "15th International OPT Workshop on Optimization for Machine Learning",
      "url": "https://opt-ml.org/"
    },
    {
      "conference": "ICML",
      "date": "2023",
      "workshop": "High-dimensional Learning Dynamics Workshop",
      "url": "https://sites.google.com/view/hidimlearning23/home"
    },
    {
      "conference": "NeurIPS",
      "date": "2022",
      "workshop": "14th International OPT Workshop on Optimization for Machine Learning",
      "url": "https://opt-ml.org/oldopt/opt22/"
    }
  ],
  "litterature_review": [
    {
      "name": "Illustrating High-Dimensional Limit Theorems for SGD via Examples",
      "date": "2023-04-29",
      "type": "review",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "This presentation provides an overview of the article <a href='https://arxiv.org/pdf/2206.04030.pdf'>“High-Dimensional Limit Theorems for SGD: Effective Dynamics and Critical Scaling”</a> by Gérard Ben Arous, Reza Gheissari, and Aukosh Jagannath. The article investigates the behavior of Stochastic Gradient Descent (SGD) with constant step-size in high-dimensional settings. The authors demonstrate that the online stochastic gradient trajectory can be approximated by a system of stochastic differential equations (SDEs). To help illustrate the main concepts of the paper, I will go through examples of applying SGD to a spiked matrix PCA problem and a two-layer network for binary Gaussian mixture classification."
    },
    {
      "name": "Using Random Matrix Models to Predict How Real-World Neural Representations Generalize",
      "date": "2022-09-14",
      "type": "review",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "How can we predict the generalization risk of overparametrized large-scale machine learning models? Even for simple but realistic regression problems, traditional theoretical analyses geared toward answering this question fail to capture some important qualitative behavior. Fortunately, the generalized cross validation (GCV) estimator can be used to accurately estimate the generalization risk. Based on the paper <a href='https://arxiv.org/abs/2203.06176'>“More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize”</a> by Wei, Hu and Steinhardt, I will explain how the GCV predicts generalization risk while other common methods fail to do the same. If time permits, I will also state some implications of this result, notably on pretraining."
    },
    {
      "name": "Hessian Eigenspectra of G-GLMs",
      "date": "2021-10-01",
      "type": "review",
      "url": "",
      "venue": "Seminar in random matrix theory, machine learning and optimization at McGill University",
      "location": "Montreal, Canada",
      "summary": "Hessian matrices are crucial to the study of optimization and machine learning algorithms. Based on <a href='https://arxiv.org/abs/2103.01519'>the work of Zhenyu Liao and Michael W. Mahoney</a>, I will present an exact asymptotic characterization of the Hessian eigenspectrum for a large family of statistical models in high dimensions. More precisely, I will introduce a limiting spectral measure theorem and discuss its implications."
    }
  ]
}
